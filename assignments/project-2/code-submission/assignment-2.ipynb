{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0fd578b525fe7fcca8a3ea11350d18bcbeb29af20bd1df15f6c5fd2c9cf111483",
   "display_name": "Python 3.9.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# COMP30027 Assignment 2\n",
    "### Lucas Fern (1080613)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Approach 1: Support Vector Machine\n",
    "This will use the `doc2vec100` dataset since SVM is capable of classification in high dimensional spaces."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import visualkeras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import ast\n",
    "\n",
    "from sklearn import svm, naive_bayes, ensemble, model_selection\n",
    "from keras.models import Sequential\n",
    "from keras import layers, Model\n",
    "from tcn import TCN, tcn_full_summary\n",
    "\n",
    "from tensorflow.keras import layers, losses, preprocessing, utils\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, metric, n_epochs):\n",
    "  plt.plot(range(1, n_epochs+1), history.history[metric])\n",
    "  plt.plot(range(1, n_epochs+1), history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.read_csv(r\"datasets/recipe_text_features_doc2vec100/train_name_doc2vec100.csv\", \n",
    "                    index_col = False, delimiter = ',', header=None)\n",
    "steps = pd.read_csv(r\"datasets/recipe_text_features_doc2vec100/train_steps_doc2vec100.csv\", \n",
    "                    index_col = False, delimiter = ',', header=None)\n",
    "ingrs = pd.read_csv(r\"datasets/recipe_text_features_doc2vec100/train_name_doc2vec100.csv\", \n",
    "                          index_col = False, delimiter = ',', header=None)\n",
    "recipes = pd.read_csv(r\"datasets/recipe_train.csv\", index_col = False, delimiter = ',')\n",
    "recipes['ingredients'] = recipes['ingredients'].apply(lambda x: ast.literal_eval(x)).apply(lambda x: ' '.join(x))\n",
    "recipes['steps'] = recipes['steps'].apply(lambda x: ast.literal_eval(x)).apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = recipes['duration_label']\n",
    "\n",
    "recipes['norm_n_ingredients'] = recipes['n_ingredients'] / max(recipes['n_ingredients'])\n",
    "n_ingrs = recipes['norm_n_ingredients']\n",
    "\n",
    "recipes['norm_n_steps'] = recipes['n_steps'] / max(recipes['n_steps'])\n",
    "n_steps = recipes['norm_n_steps']\n",
    "\n",
    "X = pd.concat([names, steps, ingrs, n_ingrs, n_steps], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='rbf', C=10)\n",
    "\n",
    "start = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Took: \" + str(time.time() - start))\n",
    "\n",
    "# Save the model since it takes a few minutes to train\n",
    "with open(\"SVM-default-testing.pkl\", \"wb\") as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved model if required\n",
    "_MODEL_FILE = r\"SVM-linear.pkl\"\n",
    "with open(_MODEL_FILE, \"rb\") as f:\n",
    "    clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "precision, recall, _, _ = precision_recall_fscore_support(predictions, y_test)\n",
    "print(precision, recall)"
   ]
  },
  {
   "source": [
    "### Generating Predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.read_csv(r\"datasets/recipe_text_features_doc2vec100/test_name_doc2vec100.csv\", \n",
    "                    index_col = False, delimiter = ',', header=None)\n",
    "steps = pd.read_csv(r\"datasets/recipe_text_features_doc2vec100/test_steps_doc2vec100.csv\", \n",
    "                    index_col = False, delimiter = ',', header=None)\n",
    "ingrs = pd.read_csv(r\"datasets/recipe_text_features_doc2vec100/test_name_doc2vec100.csv\", \n",
    "                          index_col = False, delimiter = ',', header=None)\n",
    "recipes = pd.read_csv(r\"datasets/recipe_test.csv\", index_col = False, delimiter = ',')\n",
    "recipes['ingredients'] = recipes['ingredients'].apply(lambda x: ast.literal_eval(x)).apply(lambda x: ' '.join(x))\n",
    "recipes['steps'] = recipes['steps'].apply(lambda x: ast.literal_eval(x)).apply(lambda x: ' '.join(x))\n",
    "\n",
    "recipes['norm_n_ingredients'] = recipes['n_ingredients'] / max(recipes['n_ingredients'])\n",
    "n_ingrs = recipes['norm_n_ingredients']\n",
    "\n",
    "recipes['norm_n_steps'] = recipes['n_steps'] / max(recipes['n_steps'])\n",
    "n_steps = recipes['norm_n_steps']\n",
    "\n",
    "X = pd.concat([names, steps, ingrs, n_ingrs, n_steps], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prediction-RBF-C10.csv', 'w+') as f: \n",
    "    pd.DataFrame(enumerate(predictions, start=1), columns=('id', 'duration_label')).to_csv(f, line_terminator='\\n', index=False)"
   ]
  },
  {
   "source": [
    "## Approach 2: Naive Bayes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"datasets/recipe_text_features_countvec/train_name_countvectorizer.pkl\", \"rb\") as f:\n",
    "    names_CV = pickle.load(f)\n",
    "with open(r\"datasets/recipe_text_features_countvec/train_steps_countvectorizer.pkl\", \"rb\") as f:\n",
    "    steps_CV = pickle.load(f)\n",
    "with open(r\"datasets/recipe_text_features_countvec/train_ingr_countvectorizer.pkl\", \"rb\") as f:\n",
    "    ingrs_CV = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes, X_test, y_train, y_test = model_selection.train_test_split(recipes, recipes['duration_label'], train_size=0.8)\n",
    "\n",
    "X_names = recipes['name']\n",
    "# The steps and ingredients are string formatted lists, so evaluate them to lists, then join to strings\n",
    "X_steps = recipes['steps']\n",
    "X_ingrs = recipes['ingredients']\n",
    "\n",
    "# Now vectorise the names into numeric values\n",
    "X_names = names_CV.transform(X_names).toarray()\n",
    "X_steps = steps_CV.transform(X_steps).toarray()\n",
    "X_ingrs = ingrs_CV.transform(X_ingrs).toarray()\n",
    "X_nstep = recipes['n_steps'].to_numpy().reshape(-1, 1)\n",
    "X_ningr = recipes['n_ingredients'].to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_names = naive_bayes.GaussianNB()\n",
    "gnb_steps = naive_bayes.GaussianNB()\n",
    "gnb_ingrs = naive_bayes.GaussianNB()\n",
    "gnb_nstep = naive_bayes.GaussianNB()\n",
    "gnb_ningr = naive_bayes.GaussianNB()\n",
    "\n",
    "models = (gnb_names, gnb_steps, gnb_ingrs, gnb_nstep, gnb_ningr)\n",
    "data = (X_names, X_steps, X_ingrs, X_nstep, X_ningr)\n",
    "\n",
    "for model, X in zip(models, data):\n",
    "    model.fit(X_train, y_train)\n",
    "    print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(X):\n",
    "    result = []\n",
    "    for _, instance in X.iterrows():\n",
    "        votes = {1: 0, 2: 0, 3: 0}\n",
    "\n",
    "        X_names = [instance['name']]\n",
    "        # The steps and ingredients are string formatted lists, so evaluate them to lists, then join to strings\n",
    "        X_steps = [instance['steps']]\n",
    "        X_ingrs = [instance['ingredients']]\n",
    "\n",
    "        # Now vectorise the names into numeric values\n",
    "        X_names = names_CV.transform(X_names).toarray()\n",
    "        X_steps = steps_CV.transform(X_steps).toarray()\n",
    "        X_ingrs = ingrs_CV.transform(X_ingrs).toarray()\n",
    "        X_nstep = np.array(instance['n_steps']).reshape(-1, 1)\n",
    "        X_ningr = np.array(instance['n_ingredients']).reshape(-1, 1)\n",
    "\n",
    "        # Take votes from each model\n",
    "        votes[gnb_names.predict(X_names)[0]] += 2\n",
    "        votes[gnb_steps.predict(X_steps)[0]] += 2\n",
    "        votes[gnb_ingrs.predict(X_ingrs)[0]] += 1\n",
    "        votes[gnb_nstep.predict(X_nstep)[0]] += 3\n",
    "        votes[gnb_ningr.predict(X_ningr)[0]] += 3\n",
    "\n",
    "        result.append(max(votes, key=votes.get))\n",
    "\n",
    "    return result\n",
    "        \n",
    "def ensemble_score(X, y):\n",
    "    predictions = np.array(ensemble_predict(X))\n",
    "    y = y.to_numpy()\n",
    "\n",
    "    print(predictions)\n",
    "    print(y)\n",
    "    print(np.equal(predictions, y))\n",
    "    accuracy = np.count_nonzero(np.equal(predictions, y)) / len(y)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gnb_names.score(X_names, y))\n",
    "print(gnb_steps.score(X_steps, y))\n",
    "print(gnb_ingrs.score(X_ingrs, y))\n",
    "print(gnb_nstep.score(X_nstep, y))\n",
    "print(gnb_ningr.score(X_ningr, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ensemble_score(recipes, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ensemble_predict(recipes)\n",
    "precision, recall, _, _ = precision_recall_fscore_support(predictions, y)\n",
    "print(precision, recall)"
   ]
  },
  {
   "source": [
    "### GNB Predictions\n",
    "Need to Count Vectorise the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recipes = pd.read_csv(r\"datasets/recipe_test.csv\", index_col = False, delimiter = ',')\n",
    "test_recipes['ingredients'] = test_recipes['ingredients'].apply(lambda x: ast.literal_eval(x)).apply(lambda x: ' '.join(x))\n",
    "test_recipes['steps'] = test_recipes['steps'].apply(lambda x: ast.literal_eval(x)).apply(lambda x: ' '.join(x))\n",
    "\n",
    "test_X_names = test_recipes['name']\n",
    "# The steps and ingredients are string formatted lists, so evaluate them to lists, then join to strings\n",
    "test_X_steps = test_recipes['steps']\n",
    "test_X_ingrs = test_recipes['ingredients']\n",
    "\n",
    "test_X_names = names_CV.transform(test_X_names).toarray()\n",
    "test_X_steps = steps_CV.transform(test_X_steps).toarray()\n",
    "test_X_ingrs = ingrs_CV.transform(test_X_ingrs).toarray()\n",
    "test_X_nstep = test_recipes['n_steps'].to_numpy().reshape(-1, 1)\n",
    "test_X_ningr = test_recipes['n_ingredients'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "test_data = (test_X_names, test_X_steps, test_X_ingrs, test_X_nstep, test_X_ningr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recipes = pd.read_csv(r\"datasets/recipe_test.csv\", index_col = False, delimiter = ',')\n",
    "\n",
    "predictions = ensemble_predict(test_recipes)\n",
    "with open('prediction-GNB-ensemble.csv', 'w+') as f: \n",
    "    pd.DataFrame(enumerate(predictions, start=1), columns=('id', 'duration_label')).to_csv(f, line_terminator='\\n', index=False)"
   ]
  },
  {
   "source": [
    "## Approach 3: Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = pd.read_csv(r\"datasets/recipe_train.csv\", index_col = False, delimiter = ',')\n",
    "recipes['ingredients'] = recipes['ingredients'].apply(lambda x: ast.literal_eval(x)).apply(lambda x: ' '.join(x))\n",
    "recipes['steps'] = recipes['steps'].apply(lambda x: ast.literal_eval(x)).apply(lambda x: ' '.join(x))\n",
    "\n",
    "train, test = model_selection.train_test_split(recipes, train_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i, instance in a.iterrows():\n",
    "#     label = int(instance['duration_label'])\n",
    "# \n",
    "#     string_rep = 'number steps: ' + str(instance['n_steps']) + \\\n",
    "#         '\\nnumber ingredients: ' + str(instance['n_ingredients']) + \\\n",
    "#         '\\nname: ' + instance['name'] + \\\n",
    "#         '\\ningredients: ' + instance['ingredients'] + \\\n",
    "#         '\\nsteps: ' + instance['steps']\n",
    "#  \n",
    "#     with open(f'NN-datasets/full-train/{label}/{i}.txt', 'w+') as f:\n",
    "#         f.write(string_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_BATCH_SIZE = 50\n",
    "_SEED = 42069\n",
    "\n",
    "raw_train_ds = preprocessing.text_dataset_from_directory(\n",
    "    'NN-datasets/train',\n",
    "    batch_size=_BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=_SEED)\n",
    "\n",
    "# raw_train_ds = preprocessing.text_dataset_from_directory(\n",
    "#     'NN-datasets/full-train',\n",
    "#     batch_size=_BATCH_SIZE)\n",
    "\n",
    "raw_val_ds = preprocessing.text_dataset_from_directory(\n",
    "    'NN-datasets/train',\n",
    "    batch_size=_BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=_SEED)\n",
    "\n",
    "raw_test_ds = preprocessing.text_dataset_from_directory(\n",
    "    'NN-datasets/test', \n",
    "    batch_size=_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_VOCAB_SIZE = 10000\n",
    "_MAX_SEQUENCE_LENGTH = 300\n",
    "\n",
    "int_vectorize_layer = TextVectorization(\n",
    "    max_tokens=_VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=_MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = raw_train_ds.map(lambda text, labels: text)\n",
    "int_vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return int_vectorize_layer(text), label\n",
    "\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_recipe, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Recipe\", first_recipe)\n",
    "print(\"Label\", first_label)\n",
    "\n",
    "print(\"'int' vectorized recipe:\",\n",
    "      int_vectorize_text(first_recipe, first_label)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_train_ds = raw_train_ds.map(int_vectorize_text)\n",
    "int_val_ds = raw_val_ds.map(int_vectorize_text)\n",
    "int_test_ds = raw_test_ds.map(int_vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def configure_dataset(dataset):\n",
    "  return dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "int_train_ds = configure_dataset(int_train_ds)\n",
    "int_val_ds = configure_dataset(int_val_ds)\n",
    "int_test_ds = configure_dataset(int_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(vocab_size, num_labels):\n",
    "#   model = tf.keras.Sequential([\n",
    "#       layers.Embedding(vocab_size, 128, mask_zero=True),\n",
    "#       layers.LSTM(64),\n",
    "#       layers.Dropout(0.3),\n",
    "#       layers.Dense(num_labels)\n",
    "#   ])\n",
    "#   return model\n",
    "\n",
    "def create_model(vocab_size, num_labels):\n",
    "  model = tf.keras.Sequential([\n",
    "      layers.Embedding(vocab_size, 128, mask_zero=True),\n",
    "      layers.Conv1D(128, 5, padding=\"valid\", activation=\"relu\", strides=1),\n",
    "      layers.GlobalMaxPooling1D(),\n",
    "      layers.Dense(num_labels)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_model = create_model(vocab_size=_VOCAB_SIZE + 1, num_labels=3)\n",
    "int_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# TODO: find the shapes \n",
    "# current_shape = (300, 1)\n",
    "# for model in int_model.layers:\n",
    "#     print(model, current_shape := model.compute_output_shape(current_shape))\n",
    "\n",
    "history = int_model.fit(int_train_ds, validation_data=int_val_ds, epochs=10)\n",
    "# history = int_model.fit(int_train_ds, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [int(i) for i in get_string_labels(int_model.predict(int_val_ds))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [list(j) for i, j in list(int_val_ds)]\n",
    "flat_labels = [item for sublist in labels for item in sublist]\n",
    "int_labels = [int(i + 1) for i in flat_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, _, _ = precision_recall_fscore_support(predictions, int_labels)\n",
    "print(precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_loss, int_accuracy = int_model.evaluate(int_test_ds)\n",
    "\n",
    "print(f\"Int model accuracy: {int_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy', 10)\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss', 10)\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 1, 1)\n",
    "plot_graphs(history, 'loss', 10)\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "source": [
    "### Prediction Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_string_labels(predicted_scores_batch):\n",
    "  predicted_int_labels = tf.argmax(predicted_scores_batch, axis=1)\n",
    "  predicted_labels = tf.gather(raw_train_ds.class_names, predicted_int_labels)\n",
    "  return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model = tf.keras.Sequential(\n",
    "    [int_vectorize_layer, int_model])\n",
    "\n",
    "export_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Test it with `raw_test_ds`, which yields raw strings\n",
    "print(get_string_labels(export_model.predict(['test', 'longer test of a few words', 'small test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data = []\n",
    "for i in range(10000):\n",
    "    with open(f'NN-datasets/unlabeled-test/{i}.txt') as f:\n",
    "        unlabeled_data.append(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model.predict(unlabeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_string_labels(export_model.predict(unlabeled_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = get_string_labels(export_model.predict(unlabeled_data))\n",
    "predictions = [float(i) for i in predictions.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualkeras.layered_view(export_model).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NN-datasets/prediction-CNN-128.csv', 'w+') as f: \n",
    "    pd.DataFrame(enumerate(predictions, start=1), columns=('id', 'duration_label')).to_csv(f, line_terminator='\\n', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Temporal Convolutional Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tcn_model(vocab_size, num_labels, kernel_size = 3, activation='relu', input_dim = None, \n",
    "                   output_dim=300, max_length = None, emb_matrix = None):\n",
    "    \n",
    "    inp = layers.Input(shape=(None, 300))\n",
    "    x = layers.Embedding(input_dim=vocab_size, \n",
    "                  output_dim=128,\n",
    "                  # Set the weight to be not trainable (static)\n",
    "                  trainable = False)(inp)\n",
    "    \n",
    "    x = layers.SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    x = TCN(128,dilations = [1, 2, 4], return_sequences=True, activation = activation, name = 'tcn1')(x)\n",
    "    x = TCN(64,dilations = [1, 2, 4], return_sequences=True, activation = activation, name = 'tcn2')(x)\n",
    "    \n",
    "    avg_pool = layers.GlobalAveragePooling1D()(x)\n",
    "    max_pool = layers.GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    conc = layers.concatenate([avg_pool, max_pool])\n",
    "    conc = layers.Dense(16, activation=\"relu\")(conc)\n",
    "    conc = layers.Dropout(0.1)(conc)\n",
    "    outp = layers.Dense(1, activation=\"sigmoid\")(conc)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tcn_model(vocab_size=_VOCAB_SIZE + 1, num_labels=3)\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(int_train_ds, validation_data=int_val_ds, epochs=4)"
   ]
  },
  {
   "source": [
    "### Precision and Recall Calculation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}